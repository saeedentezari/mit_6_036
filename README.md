# [mit_6_036: Introduction to Machine Learning](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwir4-qIrcf9AhUGH-wKHXM1DBAQFnoECBkQAQ&url=https%3A%2F%2Fopenlearninglibrary.mit.edu%2Fcourses%2Fcourse-v1%3AMITx%2B6.036%2B1T2019%2Fabout&usg=AOvVaw0ZduY4SPYjLnBpRvMjwgzt)

# Homework Solutions

## Week2: Perceptron

1. `perceptron`, `averaged_perceptron`, test and evaluation (cross-validation), in addition of some datasets.

2. A basic approach of object-oriented 2D-plotting of data and (linear and nonlinear) separators.


## Week3: Feature Representation

### code_and_data_for_hw3

1. `my_code_for_hw3_part1.py` (module): Polynomial transformation: implementation and test.

2. `my_code_for_hw3_part2.py` (module): Loading and representing auto (numerical), reviews (textual), and mnist (visual) data.

3. `my_hw3_part2_main.py` (analysis): Auto, reviews, and mnist data analysis.

4. `polynomial_transformation.py` (module): Polynomial Transformation Implementation by Solving Combinations with Replacement Problem.

5. `stopwords.txt`: A text file contains stopwords, arranged in lines.

### lab3_data

1. MNIST data: in `mnist` folder. Data corresponding to each particular digit is in a separate .png file.

2. Auto data: in `auto-mpg.tsv` and `small_auto_mpg.tsv` files.

3. Review data: in `reviews.tsv` and `reviews_submit.tsv` files.

> Tools for loading each of the above data are present in the `my_code_for_hw3_part2.py` file.


## Week4: Margin Maximization

1. A brief implementation of SVM.

2. Explicit (for hinge loss) and numerical implementation of Gradient Descent.


## Week5: Regression

Put the L2 regularization into the regression by Ridge Regression, and see the effect of polynomial transformation to the accuracy of the prediction.

1. `my_code_for_hw5.py` (module): Functional Tools for Ridge Regression on Auto Data.

2. `my_auto.py` (analysis): Ridge Regression on Auto Data.

`auto-mpg-regression.tsv` as data file.
